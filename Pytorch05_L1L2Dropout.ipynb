{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task5\n",
    "\n",
    "### Dropout  \n",
    "- Dropout is a regularization technique patented by Google for reducing overfitting in neural networks by preventing complex co-adaptations on training data. It is a very efficient way of performing model averaging with neural networks. The term \"dropout\" refers to dropping out units (both hidden and visible) in a neural network\n",
    "\n",
    "### L1 and L2 Regularization\n",
    "- A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression.\n",
    "<center>$L(w) = E_D(w)+ \\frac{\\lambda}{n}\\sum_{i=1}^{n}|w_i|$</center>\n",
    "- The L2 regularization used L2 norm \n",
    "<center>$L(w) = E_D(w)+ \\frac{\\lambda}{2n}\\sum_{i=1}^{n}|w_i^2|$</center>\n",
    "\n",
    "- The following numpy implementation part is referred to <a href=\"https://www.kaggle.com/mtax687/dropout-regularization-of-neural-net-using-numpy\">kaggle</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mathematically**:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1] (i)}\\tag{1}$$ \n",
    "$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$\n",
    "$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2] (i)}\\tag{3}$$\n",
    "$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$\n",
    "$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}$$\n",
    "\n",
    "Given the predictions on all the examples, you can also compute the cost $J$ as follows: \n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy to implement Dropout L1 and L2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid\n",
    "def sigmoid(z):\n",
    "    \n",
    "    s = 1.0 / (1.0 + np.exp(-1.0 * z))\n",
    "    \n",
    "    return s\n",
    "\n",
    "global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters initialization \n",
    "def initialize_parameters(x, h, y):\n",
    "    '''\n",
    "        x - input size\n",
    "        h - hidden size\n",
    "        y - output size\n",
    "        \n",
    "        params: python dictionary containing parameters\n",
    "        w1 : (h, x)\n",
    "        b1 : (h, 1)\n",
    "        w2 : (y, h)\n",
    "        b2 : (y, 1)\n",
    "    '''\n",
    "    \n",
    "    w1 = np.random.randn(h, x) * 0.01\n",
    "    b1 = np.zeros((h, 1))\n",
    "    w2 = np.random.randn(y, h) * 0.01\n",
    "    b2 = np.zeros((y, 1))\n",
    "    \n",
    "    parameters = {\n",
    "        \"w1\" : w1,\n",
    "        \"b1\" : b1,\n",
    "        \"w2\" : w2,\n",
    "        \"b2\" : b2\n",
    "    }\n",
    "    \n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters(2,4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pro_with_dropout(x, parameters, prob = 0.5):\n",
    "    '''\n",
    "        w1 : (h, x)\n",
    "        b1 : (h, 1)\n",
    "        w2 : (y, h)\n",
    "        b2 : (y, 1)\n",
    "        \n",
    "        prob: probability of keeping neuron active \n",
    "        \n",
    "        return: \n",
    "            a2: sigmoid output of the second activation \n",
    "            cache : dictionary containing z1 a1 z2 and a2\n",
    "            \n",
    "    '''\n",
    "    \n",
    "    w1 = parameters[\"w1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    w2 = parameters[\"w2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    z1 = np.add(np.matmul(w1, x), b1)\n",
    "    a1 = np.tanh(z1)\n",
    "    \n",
    "    \n",
    "    d1 = np.random.rand(a1.shape[0], a1.shape[1])\n",
    "    d1 = d1 < prob\n",
    "    \n",
    "    a1 = np.multiply(a1, d1)\n",
    "    #  By doing this you are assuring that the result of the cost \n",
    "    #  will still have the same expected value as without drop-out.\n",
    "    a1 = a1 / prob\n",
    "    \n",
    "    z2 = np.add(np.matmul(w2, a1), b2)\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    cache = {\n",
    "        \"z1\" : z1,\n",
    "        \"a1\" : a1,\n",
    "        \"d1\" : d1, \n",
    "        \"z1\" : z2,\n",
    "        \"a2\" : a2\n",
    "    }\n",
    "    \n",
    "    return a2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost J\n",
    "def compute_cost(a2, y, parameters, lamda = 0.001):\n",
    "    '''\n",
    "    a2 : output of the second activation \n",
    "    y : true labels \n",
    "    parameters: w1 b1 w2 b2\n",
    "    lambda\n",
    "    '''\n",
    "    \n",
    "    m = y.shape[1] # number of example\n",
    "    \n",
    "    logprobs = np.multiply(y, np.log(a2)) + np.multiply((1 - y), np.log(1 - a2))\n",
    "    \n",
    "    l2_reg = np.sum(np.square(parameters['w1'])) + np.sum(np.square(parameters['w2']))\n",
    "    \n",
    "    cost = (-1.0/m) * np.sum(logprobs)\n",
    "    cost = np.squeeze(cost) #makes sure cost is the dimension we expect. \n",
    "    \n",
    "    l2_cost = cost + lamda * l2_reg / (2 * m)\n",
    "    return l2_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pro_with_dropout(parameters, cache, x, y, lamda= 0.001, prob=0.5):\n",
    "    '''\n",
    "    parameters \n",
    "    cache \n",
    "    x\n",
    "    y\n",
    "    \n",
    "    \n",
    "    return grads \n",
    "    '''\n",
    "    \n",
    "    m = x.shape[1]\n",
    "    # 参数提取\n",
    "    w1 = parameters['w1']\n",
    "    w2 = parameters['w2']\n",
    "    a1 = cache['a1']\n",
    "    a2 = cache['a2']\n",
    "    d1 = cache['d1']\n",
    "    \n",
    "    dz2 = a2 - y\n",
    "    dw2 = np.dot(dz2, a1.T) / m + lamda / m * w2\n",
    "    db2 = np.sum(dz2, axis=1, keepdims=True) / m\n",
    " \n",
    "    # Dropout的关键操作\n",
    "    da1 = np.dot(w2.T, dz2)\n",
    " \n",
    "    da1 = da1 * d1\n",
    "    da1 = da1 / prob\n",
    " \n",
    "    dz1 = np.multiply(np.dot(w2.T, dz2), (1 - np.power(a1, 2)))\n",
    "    dw1 = np.dot(dz1, x.T) / m + lamda / m * w1\n",
    "    db1 = np.sum(dz1, axis=1, keepdims=True) / m\n",
    "    \n",
    "    \n",
    "    grads = {\n",
    "        \"dw1\" : dw1,\n",
    "        \"db1\" : db1,\n",
    "        \"dw2\" : dw2,\n",
    "        \"db2\" : db2\n",
    "    }\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_parameters(parameters, grads, learning_rate=0.01):\n",
    "    w1 = parameters[\"w1\"]\n",
    "    b1 = parameters['b1']\n",
    "    w2 = parameters['w2']\n",
    "    b2 = parameters['b2']\n",
    " \n",
    "    dw1 = learning_rate * grads[\"dw1\"]\n",
    "    db1 = learning_rate * grads[\"db1\"]\n",
    "    dw2 = learning_rate * grads[\"dw2\"]\n",
    "    db2 = learning_rate * grads[\"db2\"]\n",
    " \n",
    "    w1 = w1 - dw1\n",
    "    b1 = b1 - db1\n",
    "    w2 = w2 - dw2\n",
    "    b2 = b2 - db2\n",
    " \n",
    "    parameters = {\n",
    "        \"w1\": w1,\n",
    "        \"b1\": b1,\n",
    "        \"w2\": w2,\n",
    "        \"b2\": b2\n",
    "    }\n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "m = 200\n",
    "X = np.random.randn(2, m)\n",
    "Y = (1 + (2 * (X[0, :] > 0) - 1) * (2 * (X[1, :] > 0) - 1)) / 2\n",
    "Y = Y.reshape(1, X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 100\n",
    "learning_rate = 0.01\n",
    "x, h, y = 2, 30, 1\n",
    "\n",
    "parameters = initialize_parameters(x, h, y)\n",
    "w1 = parameters[\"w1\"]\n",
    "b1 = parameters[\"b1\"]\n",
    "w2 = parameters[\"w2\"]\n",
    "b2 = parameters[\"b2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w1': array([[-0.01306534,  0.0007638 ],\n",
       "        [ 0.00367232,  0.01232899],\n",
       "        [-0.00422857,  0.00086464],\n",
       "        [-0.02142467, -0.00830169],\n",
       "        [ 0.00451616,  0.01104174],\n",
       "        [-0.00281736,  0.02056356],\n",
       "        [ 0.01760249, -0.00060652],\n",
       "        [-0.02413503, -0.01777566],\n",
       "        [-0.00777859,  0.01115841],\n",
       "        [ 0.00310272, -0.02094248],\n",
       "        [-0.00228766,  0.01613361],\n",
       "        [-0.00374805, -0.0074997 ],\n",
       "        [ 0.02054624,  0.0005341 ],\n",
       "        [-0.00479157,  0.00350167],\n",
       "        [ 0.00017165, -0.00429142],\n",
       "        [ 0.01208456,  0.01115702],\n",
       "        [ 0.00840862, -0.00102887],\n",
       "        [ 0.011469  , -0.00049703],\n",
       "        [ 0.00466643,  0.01033687],\n",
       "        [ 0.00808844,  0.01789755],\n",
       "        [ 0.00451284, -0.0168406 ],\n",
       "        [-0.0116017 ,  0.01350107],\n",
       "        [-0.00331283,  0.00386539],\n",
       "        [-0.00851456,  0.01000881],\n",
       "        [-0.00384832,  0.01458108],\n",
       "        [-0.00532234,  0.01118133],\n",
       "        [ 0.00674396, -0.00722392],\n",
       "        [ 0.01098996, -0.00901634],\n",
       "        [-0.00822467,  0.00721711],\n",
       "        [-0.00625342, -0.00593843]]), 'b1': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]), 'w2': array([[-0.00343901, -0.01000169,  0.01044994,  0.00608515, -0.00069329,\n",
       "         -0.00108392,  0.00450156,  0.01765335,  0.0087097 , -0.00508457,\n",
       "          0.00777419, -0.00118771, -0.00198998,  0.01866471, -0.00418938,\n",
       "         -0.00479185, -0.01952105, -0.01402329,  0.00451123, -0.00694921,\n",
       "          0.00515414, -0.01114871, -0.0076731 ,  0.00674571,  0.01460892,\n",
       "          0.00592473,  0.01197831,  0.01704594,  0.01040089, -0.0091844 ]]), 'b2': array([[0.]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0:0.693116\n",
      "Loss after iteration 10:0.693152\n",
      "Loss after iteration 20:0.693083\n",
      "Loss after iteration 30:0.693015\n",
      "Loss after iteration 40:0.693013\n",
      "Loss after iteration 50:0.692935\n",
      "Loss after iteration 60:0.692903\n",
      "Loss after iteration 70:0.692963\n",
      "Loss after iteration 80:0.692873\n",
      "Loss after iteration 90:0.692796\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_iterations):\n",
    "    a2, cache = forward_pro_with_dropout(X, parameters)\n",
    "    \n",
    "    loss = compute_cost(a2, Y, parameters)\n",
    "    \n",
    "    grads = backward_pro_with_dropout(parameters, cache, X, Y)\n",
    "    \n",
    "    parameters = update_parameters(parameters, grads, learning_rate)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(\"Loss after iteration %i:%f\" % (i, loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170500096it [02:50, 2843384.86it/s]                               "
     ]
    }
   ],
   "source": [
    "### Pytorch: implementation of dropout\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "# cifar-10官方提供的数据集是用numpy array存储的\n",
    "# 下面这个transform会把numpy array变成torch tensor，然后把rgb值归一到[0, 1]这个区间\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# 在构建数据集的时候指定transform，就会应用我们定义好的transform\n",
    "# root是存储数据的文件夹，download=True指定如果数据不存在先下载数据\n",
    "cifar_train = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                           download=True, transform=transform)\n",
    "cifar_test = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                          transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(cifar_train, batch_size=32, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(cifar_test, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LeNet(nn.Module):\n",
    "    # 一般在__init__中定义网络需要的操作算子，比如卷积、全连接算子等等\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    # forward这个函数定义了前向传播的运算，只需要像写普通的python算数运算那样就可以了\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        # 下面这步把二维特征图变为一维，这样全连接层才能处理\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# optim中定义了各种各样的优化方法，包括SGD\n",
    "import torch.optim as optim\n",
    "\n",
    "# 如果你没有GPU，那么可以忽略device相关的代码\n",
    "device = torch.device(\"cpu\")\n",
    "net = LeNet().to(device)\n",
    "\n",
    "# CrossEntropyLoss就是我们需要的损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "[Epoch 1, Batch   100] loss: 2.305\n",
      "[Epoch 1, Batch   200] loss: 2.305\n",
      "[Epoch 1, Batch   300] loss: 2.304\n",
      "[Epoch 1, Batch   400] loss: 2.301\n",
      "[Epoch 1, Batch   500] loss: 2.299\n",
      "[Epoch 1, Batch   600] loss: 2.297\n",
      "[Epoch 1, Batch   700] loss: 2.293\n",
      "[Epoch 1, Batch   800] loss: 2.292\n",
      "[Epoch 1, Batch   900] loss: 2.283\n",
      "[Epoch 1, Batch  1000] loss: 2.270\n",
      "[Epoch 1, Batch  1100] loss: 2.265\n",
      "[Epoch 1, Batch  1200] loss: 2.237\n",
      "[Epoch 1, Batch  1300] loss: 2.202\n",
      "[Epoch 1, Batch  1400] loss: 2.176\n",
      "[Epoch 1, Batch  1500] loss: 2.155\n",
      "[Epoch 2, Batch   100] loss: 2.111\n",
      "[Epoch 2, Batch   200] loss: 2.065\n",
      "[Epoch 2, Batch   300] loss: 2.076\n",
      "[Epoch 2, Batch   400] loss: 2.077\n",
      "[Epoch 2, Batch   500] loss: 2.032\n",
      "[Epoch 2, Batch   600] loss: 2.028\n",
      "[Epoch 2, Batch   700] loss: 2.045\n",
      "[Epoch 2, Batch   800] loss: 1.993\n",
      "[Epoch 2, Batch   900] loss: 1.988\n",
      "[Epoch 2, Batch  1000] loss: 1.956\n",
      "[Epoch 2, Batch  1100] loss: 1.928\n",
      "[Epoch 2, Batch  1200] loss: 1.949\n",
      "[Epoch 2, Batch  1300] loss: 1.903\n",
      "[Epoch 2, Batch  1400] loss: 1.880\n",
      "[Epoch 2, Batch  1500] loss: 1.897\n",
      "[Epoch 3, Batch   100] loss: 1.876\n",
      "[Epoch 3, Batch   200] loss: 1.867\n",
      "[Epoch 3, Batch   300] loss: 1.845\n",
      "[Epoch 3, Batch   400] loss: 1.852\n",
      "[Epoch 3, Batch   500] loss: 1.828\n",
      "[Epoch 3, Batch   600] loss: 1.814\n",
      "[Epoch 3, Batch   700] loss: 1.826\n",
      "[Epoch 3, Batch   800] loss: 1.811\n",
      "[Epoch 3, Batch   900] loss: 1.805\n",
      "[Epoch 3, Batch  1000] loss: 1.800\n",
      "[Epoch 3, Batch  1100] loss: 1.760\n",
      "[Epoch 3, Batch  1200] loss: 1.786\n",
      "[Epoch 3, Batch  1300] loss: 1.786\n",
      "[Epoch 3, Batch  1400] loss: 1.746\n",
      "[Epoch 3, Batch  1500] loss: 1.762\n",
      "[Epoch 4, Batch   100] loss: 1.749\n",
      "[Epoch 4, Batch   200] loss: 1.773\n",
      "[Epoch 4, Batch   300] loss: 1.717\n",
      "[Epoch 4, Batch   400] loss: 1.720\n",
      "[Epoch 4, Batch   500] loss: 1.741\n",
      "[Epoch 4, Batch   600] loss: 1.725\n",
      "[Epoch 4, Batch   700] loss: 1.727\n",
      "[Epoch 4, Batch   800] loss: 1.702\n",
      "[Epoch 4, Batch   900] loss: 1.704\n",
      "[Epoch 4, Batch  1000] loss: 1.720\n",
      "[Epoch 4, Batch  1100] loss: 1.692\n",
      "[Epoch 4, Batch  1200] loss: 1.715\n",
      "[Epoch 4, Batch  1300] loss: 1.710\n",
      "[Epoch 4, Batch  1400] loss: 1.694\n",
      "[Epoch 4, Batch  1500] loss: 1.709\n",
      "[Epoch 5, Batch   100] loss: 1.669\n",
      "[Epoch 5, Batch   200] loss: 1.686\n",
      "[Epoch 5, Batch   300] loss: 1.712\n",
      "[Epoch 5, Batch   400] loss: 1.679\n",
      "[Epoch 5, Batch   500] loss: 1.657\n",
      "[Epoch 5, Batch   600] loss: 1.678\n",
      "[Epoch 5, Batch   700] loss: 1.651\n",
      "[Epoch 5, Batch   800] loss: 1.661\n",
      "[Epoch 5, Batch   900] loss: 1.673\n",
      "[Epoch 5, Batch  1000] loss: 1.636\n",
      "[Epoch 5, Batch  1100] loss: 1.625\n",
      "[Epoch 5, Batch  1200] loss: 1.654\n",
      "[Epoch 5, Batch  1300] loss: 1.620\n",
      "[Epoch 5, Batch  1400] loss: 1.650\n",
      "[Epoch 5, Batch  1500] loss: 1.620\n",
      "Done Training!\n"
     ]
    }
   ],
   "source": [
    "print(\"Start Training...\")\n",
    "for epoch in range(5):\n",
    "    # 我们用一个变量来记录每100个batch的平均loss\n",
    "    loss100 = 0.0\n",
    "    # 我们的dataloader派上了用场\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device) # 注意需要复制到GPU\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss100 += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print('[Epoch %d, Batch %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, loss100 / 100))\n",
    "            loss100 = 0.0\n",
    "\n",
    "print(\"Done Training!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
